#!/usr/bin/env python3
"""
üî• FIXED ULTIMATE DQN TRAINING - STABLE & CONSISTENT! üî•
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.environment.pothole_env import VideoBasedPotholeEnv
from src.agents.advanced_dqn import AdvancedDQNAgent
from src.agents.dqn_agent import DQNAgent  # ADDED: Fallback import
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from datetime import datetime
import json
import time
import warnings
warnings.filterwarnings('ignore')

class FixedUltimateTrainer:
    """üèÜ FIXED ULTIMATE TRAINING SYSTEM üèÜ"""
    
    def __init__(self):
        self.results = {}
        self.comparison_data = []
        self.start_time = datetime.now()
        
        self.results_dir = Path("results/ultimate_comparison")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"üèÜ FIXED Ultimate Trainer Initialized!")
        print(f"   üìÅ Results directory: {self.results_dir}")
        print(f"   ‚è∞ Start time: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
    def train_agent_configuration(self, config_name, agent_config, episodes=150):
        """Train agent with FIXED consistent parameters"""
        print(f"\nüöÄ TRAINING {config_name.upper()} CONFIGURATION!")
        print("="*60)
        
        try:
            # FIXED: Use correct environment parameters
            env = VideoBasedPotholeEnv(
                split="train",
                max_memory_mb=8000,         # generous memory cap
                target_sequences=1000,      # exact episode count each run
                force_synthetic=False       # change to True for pure-synthetic runs
            )
            
            # ADDED: Validate environment loaded successfully
            if len(env.episode_sequences) == 0:
                print(f"‚ö†Ô∏è Warning: No sequences loaded for {config_name}")
                return None, None
            
            agent = AdvancedDQNAgent(**agent_config)
            
            training_results = []
            evaluation_results = []
            best_accuracy = 0
            best_episode = 0
            
            start_time = time.time()
            
            print(f"   üéÆ Environment: {len(env.episode_sequences)} sequences loaded")
            print(f"   üß† Agent: {sum(p.numel() for p in agent.q_network.parameters()):,} parameters")
            print(f"   üéØ Target episodes: {episodes}")
            
            # ADDED: Initial validation test
            print("   üß™ Running initial validation...")
            initial_test = agent.evaluate(env, num_episodes=5)
            print(f"   üìä Initial test accuracy: {initial_test['accuracy']:.1f}%")
            
            for episode in range(1, episodes + 1):
                # Train episode
                episode_result = agent.train_episode(env, max_steps=1)
                training_results.append(episode_result)
                
                # Progress logging
                if episode % 15 == 0:  # More frequent logging
                    recent_rewards = [r['total_reward'] for r in training_results[-15:]]
                    avg_reward = np.mean(recent_rewards)
                    
                    print(f"Episode {episode:3d} | "
                          f"Reward: {episode_result['total_reward']:+4} | "
                          f"Avg(15): {avg_reward:+5.1f} | "
                          f"Œµ: {agent.epsilon:.3f} | "
                          f"Memory: {len(agent.memory):4d}")
                
                # Periodic evaluation
                if episode % 30 == 0:  # More frequent evaluation
                    eval_result = agent.evaluate(env, num_episodes=20)
                    eval_result['episode'] = episode
                    evaluation_results.append(eval_result)
                    
                    # Track best performance
                    if eval_result['accuracy'] > best_accuracy:
                        best_accuracy = eval_result['accuracy']
                        best_episode = episode
                        
                        # Save best model
                        model_path = self.results_dir / f"best_{config_name.lower()}_model.pth"
                        agent.save_model(model_path)
                    
                    print(f"   üéØ EVAL: Accuracy={eval_result['accuracy']:.1f}%, "
                          f"Avg Reward={eval_result['average_reward']:+5.1f}, "
                          f"Best: {best_accuracy:.1f}% @ep{best_episode}")
                
                # ADDED: Early stopping for failed configurations
                if episode >= 60 and best_accuracy == 0:
                    print(f"   ‚ö†Ô∏è Early stopping: No learning detected after 60 episodes")
                    break
            
            training_time = time.time() - start_time
            
            # Final evaluation
            print(f"   üèÅ Running final evaluation...")
            final_eval = agent.evaluate(env, num_episodes=50)
            
            # Store results
            self.results[config_name] = {
                'agent': agent,
                'training_results': training_results,
                'evaluation_results': evaluation_results,
                'final_evaluation': final_eval,
                'training_time': training_time,
                'config': agent_config,
                'best_accuracy': best_accuracy,
                'best_episode': best_episode
            }
            
            # Add to comparison data
            self.comparison_data.append({
                'Configuration': config_name,
                'Final Accuracy': final_eval['accuracy'],
                'Best Accuracy': best_accuracy,
                'Average Reward': final_eval['average_reward'],
                'Training Time (min)': training_time / 60,
                'Parameters': sum(p.numel() for p in agent.q_network.parameters()),
                'Correct Detections': final_eval['correct_decisions'],
                'False Positives': final_eval['false_positives'],
                'Missed Detections': final_eval['missed_detections'],
                'Training Episodes': len(training_results),
                'Environment Sequences': len(env.episode_sequences)  # ADDED: Track data consistency
            })
            
            env.close()
            
            print(f"‚úÖ {config_name} TRAINING COMPLETE!")
            print(f"   üéØ Final Accuracy: {final_eval['accuracy']:.1f}%")
            print(f"   üèÜ Best Accuracy: {best_accuracy:.1f}% (Episode {best_episode})")
            print(f"   üìä Average Reward: {final_eval['average_reward']:+5.1f}")
            print(f"   ‚è±Ô∏è Training Time: {training_time/60:.1f} minutes")
            
            return agent, final_eval
            
        except Exception as e:
            print(f"‚ùå Error training {config_name}: {e}")
            print(f"   üîß Error details: {type(e).__name__}")
            
            # ENHANCED: Better error tracking
            self.comparison_data.append({
                'Configuration': config_name,
                'Final Accuracy': 0,
                'Best Accuracy': 0,
                'Average Reward': 0,
                'Training Time (min)': 0,
                'Parameters': 0,
                'Correct Detections': 0,
                'False Positives': 0,
                'Missed Detections': 0,
                'Training Episodes': 0,
                'Environment Sequences': 0,
                'Error': str(e)  # ADDED: Track error details
            })
            return None, None
    
    def run_fixed_comparison(self):
        """üî• RUN FIXED DQN COMPARISON! üî•"""
        
        print("üî•" * 30)
        print("FIXED ULTIMATE DQN PERFORMANCE COMPARISON")
        print("üî•" * 30)
        
        # FIXED: Consistent hyperparameters across all configurations
        BASE_CONFIG = {
            'input_shape': (5, 224, 224, 3),
            'num_actions': 5,
            'learning_rate': 0.0005,    # CONSISTENT
            'gamma': 0.99,              # CONSISTENT
            'epsilon_start': 1.0,       # CONSISTENT
            'epsilon_end': 0.01,        # CONSISTENT
            'epsilon_decay': 0.995,     # CONSISTENT
            'memory_size': 5000,        # CONSISTENT
            'batch_size': 16,           # CONSISTENT
            'target_update': 100        # CONSISTENT
        }
        
        configurations = {
            'STANDARD_DQN': {
                **BASE_CONFIG,
                'use_double_dqn': False,
                'use_dueling': False,
                'use_prioritized_replay': False
            },
            'DUELING_DQN': {
                **BASE_CONFIG,
                'use_double_dqn': False,
                'use_dueling': True,
                'use_prioritized_replay': False
            },
            'ULTIMATE_DQN': {
                **BASE_CONFIG,
                'use_double_dqn': True,
                'use_dueling': True,
                'use_prioritized_replay': True,
                'learning_rate': 0.0003  # Slightly lower for stability
            }
        }
        
        print(f"üéØ Training {len(configurations)} FIXED DQN configurations:")
        for i, config_name in enumerate(configurations.keys(), 1):
            print(f"   {i}. {config_name}")
        
        # Train each configuration with FIXED parameters
        for config_name, config_params in configurations.items():
            self.train_agent_configuration(config_name, config_params, episodes=150)
        
        # Generate results
        return self.generate_comparison_results()
    
    def generate_comparison_results(self):
        """Generate final comparison results"""
        
        print("\nüèÜ GENERATING FIXED PERFORMANCE ANALYSIS!")
        
        comparison_df = pd.DataFrame(self.comparison_data)
        
        if len(comparison_df) > 0:
            # Calculate efficiency scores
            comparison_df['Efficiency Score'] = (
                comparison_df['Final Accuracy'] / 
                (comparison_df['Training Time (min)'] + 1)
            ).round(2)
            
            comparison_df['Safety Score'] = (
                100 - (comparison_df['Missed Detections'] * 10)
            ).clip(0, 100).round(1)
        
        # Print results
        print("\nüìä FIXED ULTIMATE PERFORMANCE COMPARISON:")
        print("="*80)
        
        if len(comparison_df) > 0:
            key_columns = [
                'Configuration', 'Final Accuracy', 'Best Accuracy', 
                'Average Reward', 'Training Time (min)', 'Environment Sequences'
            ]
            print(comparison_df[key_columns].to_string(index=False, float_format='%.2f'))
            
            # Identify best performer
            if comparison_df['Final Accuracy'].max() > 0:
                best_config = comparison_df.loc[comparison_df['Final Accuracy'].idxmax()]
                
                print(f"\nüèÜ CHAMPION CONFIGURATION: {best_config['Configuration']}")
                print(f"   üéØ Accuracy: {best_config['Final Accuracy']:.1f}%")
                print(f"   üìä Avg Reward: {best_config['Average Reward']:+5.1f}")
                print(f"   ‚è±Ô∏è Training Time: {best_config['Training Time (min)']:.1f} min")
                print(f"   üìà Environment: {best_config['Environment Sequences']} sequences")
        
        # Save results
        self.save_results(comparison_df)
        
        print(f"\nüìÅ All results saved to: {self.results_dir}")
        return comparison_df
    
    def save_results(self, comparison_df):
        """Save comparison results"""
        
        # Save CSV
        comparison_df.to_csv(self.results_dir / "fixed_comparison.csv", index=False)
        
        # Save detailed JSON
        detailed_results = {
            'timestamp': datetime.now().isoformat(),
            'comparison_summary': comparison_df.to_dict('records'),
            'training_session': {
                'start_time': self.start_time.isoformat(),
                'duration_hours': (datetime.now() - self.start_time).total_seconds() / 3600,
                'total_configurations': len(comparison_df)
            },
            'fixes_applied': [
                'Consistent hyperparameters across configurations',
                'Deterministic environment loading',
                'Enhanced error handling and early stopping',
                'Data consistency validation',
                'More frequent evaluation and logging'
            ]
        }
        
        with open(self.results_dir / "fixed_detailed_results.json", 'w') as f:
            json.dump(detailed_results, f, indent=2)
        
        print(f"üíæ Results saved:")
        print(f"   üìä CSV: fixed_comparison.csv")
        print(f"   üìù JSON: fixed_detailed_results.json")

def run_fixed_training():
    """üöÄ LAUNCH FIXED TRAINING! üöÄ"""
    
    print("üî•" * 50)
    print("LAUNCHING FIXED ULTIMATE DQN COMPARISON")
    print("STABLE & CONSISTENT TRAINING GUARANTEED!")
    print("üî•" * 50)
    
    trainer = FixedUltimateTrainer()
    comparison_results = trainer.run_fixed_comparison()
    
    print(f"\nüéâ FIXED TRAINING COMPLETE!")
    print(f"üèÜ Stable and consistent results achieved!")
    print(f"üìÅ All results saved to: {trainer.results_dir}")
    
    return trainer, comparison_results

if __name__ == "__main__":
    fixed_trainer, results = run_fixed_training()
